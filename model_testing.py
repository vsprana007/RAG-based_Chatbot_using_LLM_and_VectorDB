import pandas as pd
from pathlib import Path
from langchain.chains import load_chain
from langchain.chains.base import Chain
from giskard import Dataset, Model, scan, GiskardClient

import ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.vectorstores import FAISS


embeddings = OllamaEmbeddings(model="mistral")
vectorstore = FAISS.load_local("./webkul_vectorDB", embeddings)



retriever = vectorstore.as_retriever()
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define the Ollama LLM function
def ollama_llm(question, context):
    formatted_prompt = f"Question: {question}\n\nContext: {context}"
    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])
    return response['message']['content']

# Define the RAG chain
def rag_chain(question):
    retrieved_docs = retriever.invoke(question)
    formatted_context = format_docs(retrieved_docs)
    return ollama_llm(question, formatted_context)














TEXT_COLUMN_NAME = "query"
# Define a custom Giskard model wrapper for the serialization.

class FAISSRAGModel(Model):
    def model_predict(self, df: pd.DataFrame) -> pd.DataFrame:
        return df[TEXT_COLUMN_NAME].apply(lambda x: self.model({"query": x}))

    def save_model(self, path: str):
        out_dest = Path(path)
        # Save the chain object
        self.model.save(out_dest.joinpath("model.json"))

        # Save the FAISS-based retriever
        db = self.model.retriever.vectorstore
        db.save_local(out_dest.joinpath("faiss"))

    @classmethod
    def load_model(cls, path: str) -> Chain:
        src = Path(path)

        # Load the FAISS-based retriever
        db = FAISS.load_local(src.joinpath("faiss"), OllamaEmbeddings(model="mistral"))

        # Load the chain, passing the retriever
        chain = load_chain(src.joinpath("model.json"), retriever=db.as_retriever())
        return chain



# Wrap the QA chain
giskard_model = FAISSRAGModel(
    model=rag_chain,  # A prediction function that encapsulates all the data pre-processing steps and that could be executed with the dataset used by the scan.
    model_type="text_generation",  # Either regression, classification or text_generation.
    name="Webkul ai assistent",  # Optional.
    description="This model is created to answer questions about the Webkul",  # Is used to generate prompts during the scan.
    feature_names=[TEXT_COLUMN_NAME]  # Default: all columns of your dataset.
)


giskard_dataset = Dataset(pd.DataFrame({
    TEXT_COLUMN_NAME: [
        "what is webkul?",
        "what is Magento?"
    ]
}))

# Validate the wrapped model and dataset.
print(giskard_model.predict(giskard_dataset).prediction)

results = scan(giskard_model, giskard_dataset, only="hallucination")

display(results)

test_suite = results.generate_test_suite("Test suite generated by scan")
test_suite.run()

# full_results = scan(giskard_model, giskard_dataset)

# display(full_results)

# test_suite = full_results.generate_test_suite("Test suite generated by scan")
# test_suite.run()